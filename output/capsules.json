{
  "stats": {
    "totalFiles": 20,
    "totalDirectories": 6,
    "totalEdges": 0,
    "externalDependencies": [],
    "entryPoints": [
      ".nexhacks/capsules.json",
      "README.md",
      "bench.py",
      "config/eval_gpt2.py",
      "config/eval_gpt2_large.py",
      "config/eval_gpt2_medium.py",
      "config/eval_gpt2_xl.py",
      "config/finetune_shakespeare.py",
      "config/train_gpt2.py",
      "config/train_shakespeare_char.py"
    ]
  },
  "files": {
    ".nexhacks/capsules.json": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/.nexhacks/capsules.json",
      "relativePath": ".nexhacks/capsules.json",
      "name": "capsules.json",
      "lang": "json",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "{\n  \"stats\": {\n    \"totalFiles\": 20,\n    \"totalDirectories\": 6,\n    \"totalEdges\": 0,\n    \"externalDependencies\": [],\n    \"entryPoints\": [\n      \".nexhacks/capsules.json\",\n      \"README.md\",\n      \"bench.py\",\n      \"config/eval_gpt2.py\",\n      \"config/eval_gpt2_large.py\",\n      \"config/eval_gpt2_medium.py\",\n      \"config/eval_gpt2_xl.py\",\n      \"config/finetune_shakespeare.py\",\n      \"config/train_gpt2.py\",\n      \"config/train_shakespeare_char.py\"\n    ]\n  },\n  \"files\": {\n    \".nexhacks/capsules.json\": {\n      \"path\": \"/Users/wayne_tx/Desktop/nanoGPT/.nexhacks/capsules.json\",\n      \"relativePath\": \".nexhacks/capsules.json\",\n      \"name\": \"capsules.json\",\n      \"lang\": \"json\",\n      \"imports\": [],\n      \"exports\": [],\n      \"topSymbols\": [],\n      \"summaryContext\": {\n        \"functionSignatures\": [],",
        "usedBy": [],
        "dependsOn": []
      }
    },
    "README.md": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/README.md",
      "relativePath": "README.md",
      "name": "README.md",
      "lang": "markdown",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\n# nanoGPT\n\n![nanoGPT](assets/nanogpt.jpg)\n\n\n---\n\n**Update Nov 2025** nanoGPT has a new and improved cousin called [nanochat](https://github.com/karpathy/nanochat). It is very likely you meant to use/find nanochat instead. nanoGPT (this repo) is now very old and deprecated but I will leave it up for posterity.\n\n---\n\nThe simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of [minGPT](https://github.com/karpathy/minGPT) that prioritizes teeth over education. Still under active development, but currently the file `train.py` reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: `train.py` is a ~300-line boilerplate training loop and `model.py` a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.\n\n![repro124m](assets/gpt2_124M_loss.png)\n\nBecause the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).\n\n## install\n\n```\npip install torch numpy transformers datasets tiktoken wandb tqdm\n```\n\nDependencies:\n\n- [pytorch](https://pytorch.org) <3\n- [numpy](https://numpy.org/install/) <3\n-  `transformers` for huggingface transformers <3 (to load GPT-2 checkpoints)\n-  `datasets` for huggingface datasets <3 (if you want to download + preprocess OpenWebText)",
        "usedBy": [],
        "dependsOn": []
      }
    },
    "bench.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/bench.py",
      "relativePath": "bench.py",
      "name": "bench.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\"\"\"\nA much shorter version of train.py for benchmarking\n\"\"\"\nimport os\nfrom contextlib import nullcontext\nimport numpy as np\nimport time\nimport torch\nfrom model import GPTConfig, GPT\n\n# -----------------------------------------------------------------------------\nbatch_size = 12\nblock_size = 1024\nbias = False\nreal_data = True\nseed = 1337\ndevice = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\ncompile = True # use PyTorch 2.0 to compile the model to be faster\nprofile = False # use pytorch profiler, or just simple benchmarking?\nexec(open('configurator.py').read()) # overrides from command line or config file\n# -----------------------------------------------------------------------------\n\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This script functions as a streamlined version of the training logic specifically designed to benchmark the GPT model. It relies on the internal model module and standard libraries but operates independently without local dependents."
    },
    "config/eval_gpt2.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/config/eval_gpt2.py",
      "relativePath": "config/eval_gpt2.py",
      "name": "eval_gpt2.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "# evaluate the base gpt2\n# n_layer=12, n_head=12, n_embd=768\n# 124M parameters\nbatch_size = 8\neval_iters = 500 # use more iterations to get good estimate\neval_only = True\nwandb_log = False\ninit_from = 'gpt2'\n",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This file configures hyperparameters for evaluating the base GPT-2 model with 124 million parameters. It operates independently without any local dependencies or connections to other files in the codebase."
    },
    "config/eval_gpt2_large.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/config/eval_gpt2_large.py",
      "relativePath": "config/eval_gpt2_large.py",
      "name": "eval_gpt2_large.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "# evaluate the base gpt2\n# n_layer=36, n_head=20, n_embd=1280\n# 774M parameters\nbatch_size = 8\neval_iters = 500 # use more iterations to get good estimate\neval_only = True\nwandb_log = False\ninit_from = 'gpt2-large'\n",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This configuration file sets parameters for evaluating the GPT2-large model without training. It functions independently with no local dependencies or downstream connections."
    },
    "config/eval_gpt2_medium.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/config/eval_gpt2_medium.py",
      "relativePath": "config/eval_gpt2_medium.py",
      "name": "eval_gpt2_medium.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "# evaluate the base gpt2\n# n_layer=24, n_head=16, n_embd=1024\n# 350M parameters\nbatch_size = 8\neval_iters = 500 # use more iterations to get good estimate\neval_only = True\nwandb_log = False\ninit_from = 'gpt2-medium'\n",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This configuration file defines settings to evaluate the GPT-2 medium model using pre-trained weights. It operates independently with no local dependencies or other files currently utilizing it."
    },
    "config/eval_gpt2_xl.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/config/eval_gpt2_xl.py",
      "relativePath": "config/eval_gpt2_xl.py",
      "name": "eval_gpt2_xl.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "# evaluate the base gpt2\n# n_layer=48, n_head=25, n_embd=1600\n# 1558M parameters\nbatch_size = 8\neval_iters = 500 # use more iterations to get good estimate\neval_only = True\nwandb_log = False\ninit_from = 'gpt2-xl'\n",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This file defines configuration parameters for evaluating the GPT-2 XL model, including batch size and iteration settings. It operates independently within the codebase with no local dependencies or dependent modules."
    },
    "config/finetune_shakespeare.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/config/finetune_shakespeare.py",
      "relativePath": "config/finetune_shakespeare.py",
      "name": "finetune_shakespeare.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "import time\n\nout_dir = 'out-shakespeare'\neval_interval = 5\neval_iters = 40\nwandb_log = False # feel free to turn on\nwandb_project = 'shakespeare'\nwandb_run_name = 'ft-' + str(time.time())\n\ndataset = 'shakespeare'\ninit_from = 'gpt2-xl' # this is the largest GPT-2 model\n\n# only save checkpoints if the validation loss improves\nalways_save_checkpoint = False\n\n# the number of examples per iter:\n# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\nbatch_size = 1\ngradient_accumulation_steps = 32\nmax_iters = 20\n\n# finetune at constant LR\nlearning_rate = 3e-5\ndecay_lr = False\n",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This configuration file defines hyperparameters and settings for fine-tuning a GPT-2 XL model on the Shakespeare dataset. It operates as a standalone script with no local dependencies and is not currently imported by any other files."
    },
    "config/train_gpt2.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/config/train_gpt2.py",
      "relativePath": "config/train_gpt2.py",
      "name": "train_gpt2.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB\n# launch as the following (e.g. in a screen session) and wait ~5 days:\n# $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py\n\nwandb_log = True\nwandb_project = 'owt'\nwandb_run_name='gpt2-124M'\n\n# these make the total batch size be ~0.5M\n# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520\nbatch_size = 12\nblock_size = 1024\ngradient_accumulation_steps = 5 * 8\n\n# this makes total number of tokens be 300B\nmax_iters = 600000\nlr_decay_iters = 600000\n\n# eval stuff\neval_interval = 1000\neval_iters = 200\nlog_interval = 10\n\n# weight decay\nweight_decay = 1e-1\n",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This file defines hyperparameters and logging settings for training a 124M GPT-2 model. It operates as a standalone configuration file passed to the training script without any local dependencies."
    },
    "config/train_shakespeare_char.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/config/train_shakespeare_char.py",
      "relativePath": "config/train_shakespeare_char.py",
      "name": "train_shakespeare_char.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "# train a miniature character-level shakespeare model\n# good for debugging and playing on macbooks and such\n\nout_dir = 'out-shakespeare-char'\neval_interval = 250 # keep frequent because we'll overfit\neval_iters = 200\nlog_interval = 10 # don't print too too often\n\n# we expect to overfit on this small dataset, so only save when val improves\nalways_save_checkpoint = False\n\nwandb_log = False # override via command line if you like\nwandb_project = 'shakespeare-char'\nwandb_run_name = 'mini-gpt'\n\ndataset = 'shakespeare_char'\ngradient_accumulation_steps = 1\nbatch_size = 64\nblock_size = 256 # context of up to 256 previous characters\n\n# baby GPT model :)\nn_layer = 6\nn_head = 6\nn_embd = 384\ndropout = 0.2\n\nlearning_rate = 1e-3 # with baby networks can afford to go a bit higher\nmax_iters = 5000\nlr_decay_iters = 5000 # make equal to max_iters usually\nmin_lr = 1e-4 # learning_rate / 10 usually",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This file configures hyperparameters for training a small character-level model on the Shakespeare dataset to debug and observe overfitting. It functions as an isolated configuration script without local dependencies or dependents, defining output paths and logging settings for the training process."
    },
    "configurator.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/configurator.py",
      "relativePath": "configurator.py",
      "name": "configurator.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\"\"\"\nPoor Man's Configurator. Probably a terrible idea. Example usage:\n$ python train.py config/override_file.py --batch_size=32\nthis will first run config/override_file.py, then override batch_size to 32\n\nThe code in this file will be run as follows from e.g. train.py:\n>>> exec(open('configurator.py').read())\n\nSo it's not a Python module, it's just shuttling this code away from train.py\nThe code in this script then overrides the globals()\n\nI know people are not going to love this, I just really dislike configuration\ncomplexity and having to prepend config. to every single variable. If someone\ncomes up with a better simple Python solution I am all ears.\n\"\"\"\n\nimport sys\nfrom ast import literal_eval\n\nfor arg in sys.argv[1:]:\n    if '=' not in arg:\n        # assume it's the name of a config file\n        assert not arg.startswith('--')\n        config_file = arg\n        print(f\"Overriding config with {config_file}:\")\n        with open(config_file) as f:\n            print(f.read())\n        exec(open(config_file).read())\n    else:\n        # assume it's a --key=value argument",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This script provides a simple configuration mechanism by executing external files to directly override global variables without complex namespaces. It is not a standard module but is executed via `exec` within entry scripts like `train.py` to handle runtime configuration overrides."
    },
    "data/openwebtext/prepare.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/data/openwebtext/prepare.py",
      "relativePath": "data/openwebtext/prepare.py",
      "name": "prepare.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "# saves the openwebtext dataset to a binary file for training. following was helpful:\n# https://github.com/HazyResearch/flash-attention/blob/main/training/src/datamodules/language_modeling_hf.py\n\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport tiktoken\nfrom datasets import load_dataset # huggingface datasets\n\n# number of workers in .map() call\n# good number to use is ~order number of cpu cores // 2\nnum_proc = 8\n\n# number of workers in load_dataset() call\n# best number might be different from num_proc above as it also depends on NW speed.\n# it is better than 1 usually though\nnum_proc_load_dataset = num_proc\n\nenc = tiktoken.get_encoding(\"gpt2\")\n\nif __name__ == '__main__':\n    # takes 54GB in huggingface .cache dir, about 8M documents (8,013,769)\n    dataset = load_dataset(\"openwebtext\", num_proc=num_proc_load_dataset)\n\n    # owt by default only contains the 'train' split, so create a test split\n    split_dataset = dataset[\"train\"].train_test_split(test_size=0.0005, seed=2357, shuffle=True)\n    split_dataset['val'] = split_dataset.pop('test') # rename the test split to val\n\n    # this results in:\n    # >>> split_dataset",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This script prepares and saves the OpenWebText dataset to a binary file for training. It operates independently with no local dependencies or dependents."
    },
    "data/openwebtext/readme.md": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/data/openwebtext/readme.md",
      "relativePath": "data/openwebtext/readme.md",
      "name": "readme.md",
      "lang": "markdown",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\n## openwebtext dataset\n\nafter running `prepare.py` (preprocess) we get:\n\n- train.bin is ~17GB, val.bin ~8.5MB\n- train has ~9B tokens (9,035,582,198)\n- val has ~4M tokens (4,434,897)\n\nthis came from 8,013,769 documents in total.\n\nreferences:\n\n- OpenAI's WebText dataset is discussed in [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n- [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset\n",
        "usedBy": [],
        "dependsOn": []
      }
    },
    "data/shakespeare/prepare.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/data/shakespeare/prepare.py",
      "relativePath": "data/shakespeare/prepare.py",
      "name": "prepare.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "import os\nimport requests\nimport tiktoken\nimport numpy as np\n\n# download the tiny shakespeare dataset\ninput_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')\nif not os.path.exists(input_file_path):\n    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n    with open(input_file_path, 'w', encoding='utf-8') as f:\n        f.write(requests.get(data_url).text)\n\nwith open(input_file_path, 'r', encoding='utf-8') as f:\n    data = f.read()\nn = len(data)\ntrain_data = data[:int(n*0.9)]\nval_data = data[int(n*0.9):]\n\n# encode with tiktoken gpt2 bpe\nenc = tiktoken.get_encoding(\"gpt2\")\ntrain_ids = enc.encode_ordinary(train_data)\nval_ids = enc.encode_ordinary(val_data)\nprint(f\"train has {len(train_ids):,} tokens\")\nprint(f\"val has {len(val_ids):,} tokens\")\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))\nval_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This script downloads the tiny Shakespeare dataset and reads the raw text data for processing. It functions as a standalone utility with no local dependencies, exports, or dependent files."
    },
    "data/shakespeare/readme.md": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/data/shakespeare/readme.md",
      "relativePath": "data/shakespeare/readme.md",
      "name": "readme.md",
      "lang": "markdown",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\n# tiny shakespeare\n\nTiny shakespeare, of the good old char-rnn fame :)\n\nAfter running `prepare.py`:\n\n- train.bin has 301,966 tokens\n- val.bin has 36,059 tokens\n",
        "usedBy": [],
        "dependsOn": []
      }
    },
    "data/shakespeare_char/prepare.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/data/shakespeare_char/prepare.py",
      "relativePath": "data/shakespeare_char/prepare.py",
      "name": "prepare.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\"\"\"\nPrepare the Shakespeare dataset for character-level language modeling.\nSo instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\nWill save train.bin, val.bin containing the ids, and meta.pkl containing the\nencoder and decoder and some other related info.\n\"\"\"\nimport os\nimport pickle\nimport requests\nimport numpy as np\n\n# download the tiny shakespeare dataset\ninput_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')\nif not os.path.exists(input_file_path):\n    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n    with open(input_file_path, 'w') as f:\n        f.write(requests.get(data_url).text)\n\nwith open(input_file_path, 'r') as f:\n    data = f.read()\nprint(f\"length of dataset in characters: {len(data):,}\")\n\n# get all the unique characters that occur in this text\nchars = sorted(list(set(data)))\nvocab_size = len(chars)\nprint(\"all the unique characters:\", ''.join(chars))\nprint(f\"vocab size: {vocab_size:,}\")\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This script prepares the tiny Shakespeare dataset for character-level language modeling by downloading the text and converting it into binary train and validation files. It operates as a standalone utility with no local dependencies or dependents to generate the necessary data artifacts."
    },
    "data/shakespeare_char/readme.md": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/data/shakespeare_char/readme.md",
      "relativePath": "data/shakespeare_char/readme.md",
      "name": "readme.md",
      "lang": "markdown",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\n# tiny shakespeare, character-level\n\nTiny shakespeare, of the good old char-rnn fame :) Treated on character-level.\n\nAfter running `prepare.py`:\n\n- train.bin has 1,003,854 tokens\n- val.bin has 111,540 tokens\n",
        "usedBy": [],
        "dependsOn": []
      }
    },
    "model.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/model.py",
      "relativePath": "model.py",
      "name": "model.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\"\"\"\nFull definition of a GPT Language Model, all of it in this single file.\nReferences:\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\n2) huggingface/transformers PyTorch implementation:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n\"\"\"\n\nimport math\nimport inspect\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This file provides a complete implementation of a GPT Language Model. It operates independently with no local dependencies or connections to other codebase modules."
    },
    "sample.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/sample.py",
      "relativePath": "sample.py",
      "name": "sample.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\"\"\"\nSample from a trained model\n\"\"\"\nimport os\nimport pickle\nfrom contextlib import nullcontext\nimport torch\nimport tiktoken\nfrom model import GPTConfig, GPT\n\n# -----------------------------------------------------------------------------\ninit_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\nout_dir = 'out' # ignored if init_from is not 'resume'\nstart = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\nnum_samples = 10 # number of samples to draw\nmax_new_tokens = 500 # number of tokens generated in each sample\ntemperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\ntop_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\nseed = 1337\ndevice = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\ncompile = False # use PyTorch 2.0 to compile the model to be faster\nexec(open('configurator.py').read()) # overrides from command line or config file\n# -----------------------------------------------------------------------------\n\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This script generates text samples from a trained GPT model by loading configurations and defining sampling parameters. It imports architecture definitions from the model module but functions as a standalone script with no exports."
    },
    "train.py": {
      "path": "/Users/wayne_tx/Desktop/nanoGPT/train.py",
      "relativePath": "train.py",
      "name": "train.py",
      "lang": "python",
      "imports": [],
      "exports": [],
      "topSymbols": [],
      "summaryContext": {
        "functionSignatures": [],
        "firstNLines": "\"\"\"\nThis training script can be run both on a single gpu in debug mode,\nand also in a larger training run with distributed data parallel (ddp).\n\nTo run on a single GPU, example:\n$ python train.py --batch_size=32 --compile=False\n\nTo run with DDP on 4 gpus on 1 node, example:\n$ torchrun --standalone --nproc_per_node=4 train.py\n\nTo run with DDP on 4 gpus across 2 nodes, example:\n- Run on the first (master) node with example IP 123.456.123.456:\n$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n- Run on the worker node:\n$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n\"\"\"\n\nimport os\nimport time\nimport math\nimport pickle\nfrom contextlib import nullcontext\n\nimport numpy as np\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\n\nfrom model import GPTConfig, GPT",
        "usedBy": [],
        "dependsOn": []
      },
      "summary": "This script trains a model and supports execution on both single GPUs and distributed data parallel environments. It functions as a standalone entry point with no local dependencies or dependents."
    }
  }
}